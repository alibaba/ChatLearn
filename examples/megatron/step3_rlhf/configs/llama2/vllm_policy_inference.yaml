includes:
  - base_inference.yaml
  - policy_shared.yaml


vllm_micro_batch_size: ${vllm_micro_batch_size:-1}

# sampling params
n: 1
ignore_eos: ${policy_ignore_eos:False}
top_p: ${policy_top_p:1.0}
top_k: ${policy_top_k:1}
temperature: ${policy_temperature:1.0}
use_beam_search: ${policy_use_beam_search:False}

eval_temperature: 0.01
use_attn_acc: ${use_attn_acc:False}
eval_top_k: 1
eval_top_p: 0

# scheduler config
max_num_batched_tokens: 32768
max_paddings: 512

# cache config
block_size: 16
gpu_memory_utilization: 0.9
swap_space : 4
sliding_window: None

tokenizer: ${tokenizer_load}