llama: True
tokenizer_type: LLAMATokenizer
untie_embeddings_and_output_weights: True
vocab_file: ${vocab_file}
num_layers: 40
hidden_size: 5120
num_attention_heads: 40
max_position_embeddings: 1024
bf16: True
seq_length: 1024
out_seq_length: 1024
fix_kl_coef: ${fix_kl_coef:True}
log_dir: ${log_dir}
exp_name: ${exp_name:test}
tensorboard_dir: ${tensorboard_dir}
loss_on_prompts: ${loss_on_prompts:False}
numerical_stable: True

build_path: ${build_path:build}


init_kl_coef: ${init_kl_coef:0.02}
target: 6
horizon: 10000
gamma: 1
lam: 0.95
cliprange: 0.2
cliprange_value: ${cliprange_value:0.1}
scale_reward: "None"

cliprange_reward: 100

max_new_tokens: 512


ngram_coef: ${ngram_coef:1}
lm_coef: ${lm_coef:0}
math_coef: ${math_coef:0}
raw_reward_coeff: ${raw_reward_coeff:1}

clipped_value_only: ${clipped_value_only:1}
num_inference_per_prompt: ${num_inference_per_prompt:1}

continue_train: ${continue_train:0}
continue_train_global_batch_size: ${continue_train_global_batch_size:-1}
continue_inference_instances: ${continue_inference_instances:-1}
continue_inference_batch_size: ${continue_inference_batch_size:-1}

save: ${save_dir}
save_interval: 1000
gradient_accumulation_fusion: 0
max_tokens_to_oom: 99999999


hysteresis: 2
use_flash_attn: 1
make_ffn_dim_multiple_of: 256
pos_emb: rotary
make_vocab_size_divisible_by: 32
do_math_eval: 0
log_entropy: False
adaptive_parallel_strategy_on_checkpoint: True
log_interval: 1
distributed_timeout_minutes: 30
