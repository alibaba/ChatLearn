max_position_embeddings: ${max_seq_len}
tokenizer_type: GPT2BPETokenizer
vocab_file: ${vocab_file}
merge_file: ${merge_file}
bf16: True
seq_length: ${max_seq_len}
out_seq_length: ${max_seq_len}
fix_kl_coef: ${fix_kl_coef:True}

log_dir: ${log_dir}

exp_name: ${exp_name:test}

loss_on_prompts: ${loss_on_prompts:False}
policy_numerical_stable: ${policy_numerical_stable:False}

build_path: ${build_path:build}



init_kl_coef: 0.02
target: 6
horizon: 10000
gamma: 1
lam: 0.95
cliprange: 0.2
cliprange_value: 0.2
scale_reward: "None"

cliprange_reward: 100

max_new_tokens: ${max_new_tokens}



ngram_coef: 0
lm_coef: 1
clipped_value_only: True
use_flash_attn: True
continue_train: ${continue_train:0}
num_inference_per_prompt: ${num_inference_per_prompt:1}
max_tokens_to_oom: 9999999999
log_entropy: False
log_interval: 0
do_math_eval: 0
math_coef: ${math_coef:0}
raw_reward_coeff: ${raw_reward_coeff:1}

# for benchmark only
use_eod_token_for_early_termination: ${use_eod_token_for_early_termination:True}
attention_dropout: 0
hidden_dropout: 0
distributed_timeout_minutes: 30
