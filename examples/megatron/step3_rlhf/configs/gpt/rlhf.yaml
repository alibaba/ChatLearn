runtime_env:
    platform: DLC
    excludes:
        - "*pt"
        - "logs"
        - "tensorboards"
        - ".nfs*"
        - "mypipe"
        - "*.pt.trace.json"
        - "*.nsys-rep"

models:
    policy:
        model_config_file: old_policy_inference.yaml
        num_device: ${num_device_policy}
        gpu_per_process: 1
        trainable: False
        generation_batch_size: ${policy_generation_batch_size:180}
        batch_generation:
            ranking: ${batch_generation_ranking:False}
            min_prompt_length: ${batch_generation_min_prompt_length:0}
    
    reference:
        model_config_file: reference.yaml
        num_device: ${num_device_ref}
        trainable: False
        generation_batch_size: ${ref_generation_bs:16}

    reward:
        model_config_file: reward_inference.yaml
        num_device: ${num_device_reward}
        generation_batch_size: ${reward_generation_bs:16}
        trainable: False
    
    value:
        model_config_file: old_value_inference.yaml
        num_device: ${num_device_value}
        generation_batch_size: ${value_generation_bs:16}
        trainable: False
    
    ppo_policy:
        model_config_file: ppo_policy.yaml
        num_device: ${num_device_ppo_policy}
        trainable: True
        lora:
            enable_lora: ${enable_lora_policy:False}
            lora_dim: 64
            lora_layer: ColumnParallelLinear,LinearLayer,RowParallelLinear
            column_only_qkv: False
            lora_dropout: 0.05
        offload_optimizer_states: ${offload_optimizer_states:False}
    
    ppo_value:
        model_config_file: ppo_value.yaml
        num_device: ${num_device_ppo_value}
        gpu_per_process: 1
        trainable: True

        lora:
            enable_lora: ${enable_lora_value:False}
            lora_dim: 64
            lora_layer: ColumnParallelLinear,LinearLayer,RowParallelLinear
            column_only_qkv: False
            lora_dropout: 0.05
        offload_optimizer_states: ${offload_optimizer_states:False}

rlhf:
    colocation:
            - policy,reference,reward,value,ppo_policy,ppo_value
    generation_batch_size: ${generation_batch_size:16}
    train_micro_batch_size: ${train_micro_batch_size:16}
    train_global_batch_size: ${train_global_batch_size:64}
    num_ppo_episode: ${num_ppo_episode:5}
    sample_per_episode: ${sample_per_episode:1024}
    num_training_epoch: 1
    save_episode_interval: ${save_episode_interval:1000}
    query_key: ${query_key:query}
    data_path: ${data_path:/path/to/data}
    training_data_num_limit: ${training_data_num_limit:-1}
    eval_episode_interval: ${eval_episode_interval:0}
    mini_data: ${mini_data:/path/to/mini.txt}
    eval_data_path: ${eval_data_path:/path/to/eval_data}
    eval_output_dir: ${eval_output_dir}
    eval_data_num_limit: 20
    nsys: False
