# bloom config
add_position_embedding: False
use_alibi_position_embeddings: True
embed_layernorm: True
untie_embeddings_and_output_weights: True
tokenizer_type: AutoTokenizer
vocab_file: ${vocab_file}
max_position_embeddings: 2048
bf16: True
seq_length: 1536
out_seq_length: 1024
fix_kl_coef: ${fix_kl_coef:True}
log_dir: ${log_dir}
exp_name: ${exp_name:test}
tensorboard_dir: ${tensorboard_dir}
loss_on_prompts: ${loss_on_prompts:False}
numerical_stable: False

build_path: ${build_path:build}


init_kl_coef: ${init_kl_coef:0.02}
target: 6
horizon: 10000
gamma: 1
lam: 0.95
cliprange: 0.2
cliprange_value: ${cliprange_value:0.1}
scale_reward: "None"

cliprange_reward: 100

max_new_tokens: 512


ngram_coef: ${ngram_coef:1}
lm_coef: ${lm_coef:0}
math_coef: ${math_coef:0}
raw_reward_coeff: ${raw_reward_coeff:1}

clipped_value_only: ${clipped_value_only:1}
num_inference_per_prompt: ${num_inference_per_prompt:1}


save: ${save_dir}
save_interval: 1000
gradient_accumulation_fusion: 0
max_tokens_to_oom: 99999999


hysteresis: 2
use_flash_attn: 0
make_ffn_dim_multiple_of: 256
pos_emb: alibi
make_vocab_size_divisible_by: 128
do_math_eval: 0
log_entropy: False
adaptive_parallel_strategy_on_checkpoint: True
log_interval: 1
distributed_timeout_minutes: 30
