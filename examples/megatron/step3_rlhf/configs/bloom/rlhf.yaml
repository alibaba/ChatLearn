runtime_env:
  platform: DLC
  excludes:
    - "*pt"
    - "logs"
    - "tensorboards"
    - ".nfs*"


models:
  policy:
    model_config_file: old_policy_inference.yaml
    num_device: ${num_device:8}
    trainable: False
    batch_generation:
      ranking: ${batch_generation_ranking:False}
      min_prompt_length: ${batch_generation_min_prompt_length:0}

  reference:
    model_config_file: reference.yaml
    num_device: ${num_device:8}
    trainable: False
    generation_batch_size: ${ref_generation_batch_size:4}

  reward:
    model_config_file: reward_inference.yaml
    num_device: ${num_device:8}
    trainable: False

  value:
    model_config_file: old_value_inference.yaml
    num_device: ${num_device:8}
    trainable: False

  ppo_policy:
    model_config_file: ppo_policy.yaml
    num_device: ${num_device:8}
    trainable: True
    lora:
      enable_lora: ${enable_lora_policy:False}
      lora_dim: 64
      lora_layer: ColumnParallelLinear,LinearLayer,RowParallelLinear
      column_only_qkv: False
      lora_dropout: 0.05

  ppo_value:
    model_config_file: ppo_value.yaml
    num_device: ${num_device:8}
    trainable: True
    lora:
      enable_lora: ${enable_lora_value:False}
      lora_dim: 64
      lora_layer: ColumnParallelLinear,LinearLayer,RowParallelLinear
      column_only_qkv: False
      lora_dropout: 0.05

rlhf:
  colocation:
    - policy,ppo_policy,reward,reference,value,ppo_value
  generation_batch_size: ${generation_batch_size:8}
  train_micro_batch_size: 2
  train_global_batch_size: ${train_global_batch_size:512}
  num_ppo_episode: 5000
  sample_per_episode: ${sample_per_episode:1024}
  num_training_epoch: 1
  save_episode_interval: ${save_episode_interval:100}
  data_path: ${data_path}
  training_data_num_limit: ${training_data_num_limit:-1}
  eval_data_num_limit: ${eval_data_num_limit:128}
  eval_episode_interval: ${eval_episode_interval:100}
  data_checkpoint_path: ${data_checkpoint_path}
  eval_output_dir: ${eval_output_dir}
