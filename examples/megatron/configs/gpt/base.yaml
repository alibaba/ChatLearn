max_position_embeddings: ${max_seq_len}
tokenizer_type: GPT2BPETokenizer
vocab_file: ${vocab_file}
merge_file: ${merge_file}
bf16: True
seq_length: ${max_seq_len}
fix_kl_coef: ${fix_kl_coef:True}

loss_on_prompts: ${loss_on_prompts:False}
numerical_stable: ${policy_numerical_stable:False}

build_path: ${build_path:build}


init_kl_coef: 0.02
target: 6
horizon: 10000
gamma: 1
lam: 0.95
cliprange: 0.2
cliprange_value: 0.2
scale_reward: "None"

cliprange_reward: 100

max_new_tokens: ${max_new_tokens}


ngram_coef: 0
lm_coef: 1
clipped_value_only: True
use_flash_attn: True
num_inference_per_prompt: ${num_inference_per_prompt:1}
train_to_compare_num_responses: ${train_to_compare_num_responses:1}
max_tokens_to_oom: 9999999999
log_entropy: False
log_interval: 0
do_math_eval: 0
math_coef: ${math_coef:0}
raw_reward_coeff: ${raw_reward_coeff:1}

# for benchmark only
use_eod_token_for_early_termination: ${use_eod_token_for_early_termination:True}
attention_dropout: 0
hidden_dropout: 0
distributed_timeout_minutes: 30
adaptive_parallel_strategy_on_checkpoint: True

trainer_engine: ${trainer_engine:rlhf}
attention_softmax_in_fp32: True
transformer_impl: local
