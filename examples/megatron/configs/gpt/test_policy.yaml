runtime_env:
    platform: DLC
    excludes:
        - "*pt"
        - "logs"
        - "tensorboards"
        - ".nfs*"
models:
    policy:
        model_config_file: old_policy_inference.yaml
        num_gpu: ${num_gpu:8}
        trainable: False
        batch_generation:
            ranking: ${batch_generation_ranking:False}
            min_prompt_length: ${batch_generation_min_prompt_length:0}
runtime:
    generation_batch_size: ${generation_batch_size:4}
    query_key: ${query_key:query}
    data_path: ${data_path:/path/to/data}
    eval_data_path: ${eval_data_path:/path/to/eval_data}
    output_dir: ${output_dir:/path/to/output_dir}
    profiler_dir: ${profiler_dir}
