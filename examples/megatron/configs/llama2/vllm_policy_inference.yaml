includes:
  - base_inference.yaml
  - policy_shared.yaml
  - data.yaml


# sampling params
n: 1
ignore_eos: ${policy_ignore_eos:False}
top_p: ${policy_top_p:0.9}
top_k: ${policy_top_k:-1}
min_p: ${policy_min_p:0.0}
temperature: ${policy_temperature:1.0}
use_beam_search: ${policy_use_beam_search:False}

eval_temperature: ${eval_temperature:1.0}
eval_top_k: ${eval_top_k:-1}
eval_top_p: ${eval_top_p:0.9}
eval_min_p: ${eval_min_p:0.0}

apply_replica_id_to_seed: ${apply_replica_id_to_seed:True}

# sample config
# stop_token_list: stop token string list, not token ids.
stop_token_list: ${stop_token_list:<EOS>;</s>}
new_token_limit: ${new_token_limit:True}
prompt_logprobs: ${prompt_logprobs:None}
skip_special_tokens: ${skip_special_tokens:True}

# scheduler config
max_num_batched_tokens: ${max_num_batched_tokens:32768}
max_paddings: ${max_paddings:512}
num_scheduler_steps: ${num_scheduler_steps:1}

# engine config
enforce_eager: ${enforce_eager:True}

# cache config
block_size: ${cache_block_size:16}
gpu_memory_utilization: ${gpu_memory_utilization:0.5}
swap_space: ${swap_space:16}
preemption_mode: ${preemption_mode:'recompute'}
sliding_window: ${sliding_window:None}
max_seq_len_to_capture: ${max_seq_len_to_capture:32768}

tokenizer: ${tokenizer_load}

vllm_input_ids_key: input_ids
vllm_prompt_key: prompt

tensor_model_parallel_size: ${policy_tp}
pipeline_model_parallel_size: ${policy_pp}

vllm_load_format: ${vllm_load_format:dummy}
apply_replica_id_to_seed: ${apply_replica_id_to_seed:True}

drop_last: ${drop_last:False}
