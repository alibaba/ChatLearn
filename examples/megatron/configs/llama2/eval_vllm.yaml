runtime_env:
  platform: DLC
  excludes:
    - "*pt"
    - "logs"
    - "tensorboards"
    - ".nfs*"


models:
  policy:
    model_config_file: vllm_policy_inference.yaml
    num_gpu: ${num_gpu_policy:16}
    trainable: False
    batch_generation:
      ranking: ${batch_generation_ranking:False}
      min_prompt_length: ${batch_generation_min_prompt_length:0}
    free_memory: ${free_memory_policy:False}

  reward:
    model_config_file: reward_inference.yaml
    num_gpu: ${num_gpu_reward:16}
    trainable: False
    free_memory: ${free_memory_reward:False}

runtime:
  colocation:
    - policy,reward
  generation_batch_size: ${generation_batch_size:4}
  save_episode_interval: ${save_episode_interval:100}
  eval_data_path: ${eval_data_path}
  eval_data_num_limit: ${eval_data_num_limit:128}
  eval_episode_interval: ${eval_episode_interval:100}
  data_checkpoint_path: ${data_checkpoint_path}
  output_dir: ${output_dir}
  free_sync_collective_group: ${free_sync_collective_group:False}
  exp_name: ${exp_name:chatlearn}
