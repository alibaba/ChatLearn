runtime_env:
  platform: DLC
  excludes:
    - "*pt"
    - "logs"
    - "tensorboards"
    - ".nfs*"


models:
  policy:
    model_config_file: old_policy_inference.yaml
    num_gpu: ${num_gpu:1}
    gpu_per_process: 1
    trainable: False
    batch_generation:
      ranking: ${batch_generation_ranking:False}
      min_prompt_length: ${batch_generation_min_prompt_length:0}

runtime:
  generation_batch_size: ${generation_batch_size:4}
  data_path: ${data_path}
  eval_data_path: ${eval_data_path}
  output_dir: ${output_dir}
  exp_name: ${exp_name:chatlearn}
