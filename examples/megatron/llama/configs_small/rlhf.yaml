runtime_env:
  platform: DLC
  excludes:
    - "*pt"
    - "logs"
    - "tensorboards"
    - ".nfs*"


models:
  policy:
    model_config_file: old_policy_inference.yaml
    num_device: 8
    gpu_per_process: 1
    trainable: False

  reference:
    model_config_file: reference.yaml
    num_device: 8
    gpu_per_process: 1
    trainable: False
    generation_batch_size: ${ref_generation_batch_size:4}

  reward:
    model_config_file: reward_inference.yaml
    num_device: 8
    gpu_per_process: 1
    trainable: False

  value:
    model_config_file: old_value_inference.yaml
    num_device: 8
    gpu_per_process: 1
    trainable: False

  ppo_policy:
    model_config_file: ppo_policy.yaml
    num_device: 8
    gpu_per_process: 1
    trainable: True

  ppo_value:
    model_config_file: ppo_value.yaml
    num_device: 8
    gpu_per_process: 1
    trainable: True

rlhf:
  colocation:
    - policy,ppo_policy,reward,reference,value,ppo_value
  generation_batch_size: ${generation_batch_size:16}
  train_micro_batch_size: 4
  train_global_batch_size: ${train_global_batch_size:32}
  num_ppo_episode: 5000
  sample_per_episode: ${sample_per_episode:32}
  num_training_epoch: 1
  save_episode_interval: ${save_episode_interval:100}
  data_path: ${data_path:/mnt/shared/Group-m6/tianhang_zhu/move/all_rlhf_data/prompt_rlhf_v3_20w_train_split_08_42.json}
  training_data_num_limit: ${training_data_num_limit:-1}
  eval_data_num_limit: ${eval_data_num_limit:-1}
  eval_episode_interval: ${eval_episode_interval:100}
  data_checkpoint_path: ${data_checkpoint_path:/mnt/user/E-tianhang.zhu-364849/rlhf_framework/Megatron-v3/data_checkpoint/default}
  eval_data_path: ${eval_data_path:/mnt/shared/Group-m6/tianhang_zhu/move/eval_data/mini.txt}
  eval_output_dir: ${eval_output_dir:/mnt/user/E-tianhang.zhu-364849/rlhf_framework_QWen/eval_dir}
