num_layers: 40
hidden_size: 5120
num_attention_heads: 40
max_position_embeddings: 1024
ffn_hidden_size: 13824
llama: True
untie_embeddings_and_output_weights: True
tokenizer_type: LLAMATokenizer
vocab_file: /cpfs01/shared/Group-m6/xianyan.xianyanjia/llama/megatron_models/vicuna-13b-new/
# merge_file: "/mnt/user/E-xianyan.xianyanjia-189885/QWen/gpt2-zhcn3-v4.bpe"
bf16: True
#fp16: True
seq_length: 1024
out_seq_length: 1024
fix_kl_coef: ${fix_kl_coef:True}
log_dir: /mnt/user/E-xianyan.xianyanjia-189885/logs
exp_name: ${exp_name:test}
tensorboard_dir: /mnt/user/E-xianyan.xianyanjia-189885/tensorboards/
loss_on_prompts: ${loss_on_prompts:False}
#numerical_stable: True

build_path: ${build_path:build}


init_kl_coef: ${init_kl_coef:0.02}
target: 6
horizon: 10000
gamma: 1
lam: 0.95
cliprange: 0.2
cliprange_value: ${cliprange_value:0.1}
scale_reward: "None"

cliprange_reward: 100

max_new_tokens: 512


ngram_coef: ${ngram_coef:1}
lm_coef: ${lm_coef:1}
clipped_value_only: ${clipped_value_only:1}
num_inference_per_prompt: ${num_inference_per_prompt:1}

continue_train: ${continue_train:0}
continue_train_global_batch_size: ${continue_train_global_batch_size:-1}
continue_inference_instances: ${continue_inference_instances:-1}
continue_inference_batch_size: ${continue_inference_batch_size:-1}

save: "/mnt/user/E-xianyan.xianyanjia-189885/save_models/"
save_interval: 1000
gradient_accumulation_fusion: 0
max_tokens_to_oom: 99999999


hysteresis: 2
use_flash_attn: 1
#make_ffn_dim_multiple_of: 256
#activation: geglu
pos_emb: rotary
#use_bladnn: True
log_entropy: False
use_distributed_optimizer: True
make_vocab_size_divisible_by: 32
use_llama_rotary: False
