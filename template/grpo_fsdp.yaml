runtime_args:
  # setup config
  train_backend: fsdp
  rollout_backend: vllm
  exp_name: grpo_fsdp
  colocation: [policy,policy_trainer,ref_policy]
  # path config
  output_dir: your_output_dir
  data_path: your_data_path
  eval_data_path: your_eval_data_path
  data_checkpoint_path: ${runtime_args.output_dir}/data_checkpoint_path/
  # config for training
  num_episode: 200
  sample_per_episode: 512
  train_micro_batch_size: 8
  train_global_batch_size: 512
  save_episode_interval: 200
  # config for data
  data_shuffle: True
  data_rerank: True
  max_replay_episode: 2
  # config for eval
  eval_episode_interval: 5
  enable_eval_before_training: False
  log_args_dict:
    log_dir: ${runtime_args.output_dir}
    enable_wandb: False
    wandb_project: your_wandb_project
    wandb_dir: ${runtime_args.output_dir}
    wandb_name: ${runtime_args.exp_name}
    wandb_id: ${runtime_args.exp_name}
    wandb_resume: allow

models:
  policy_trainer:
    use_expandable_segments: False
    free_gpu_memory:
      offload_weights: True
      offload_optimizer_states: True
      free_grad_buffers: True
    optimizer:
      lr: 2e-6
      clip_grad: 1
    trainable: True
    generation_batch_size: 8
    train_micro_batch_size: ${runtime_args.train_micro_batch_size}
    gpu_per_process: 1
    num_gpu: 2
    fsdp_size: ${models.policy_trainer.num_gpu}
    ulysses_sequence_parallel_size: 1
    packing: False
    meta_init: False
    groupgemm: False
    max_token_in_packing: 32768
    load: your_hf_model_path
    gradient_checkpointing: True
    pos_clip_ratio: 0.2
    neg_clip_ratio: 0.2
    entropy_coef: 0.0
    kl_coef: 0.0
    save_hf: True
  ref_policy:
    use_expandable_segments: ${models.policy_trainer.use_expandable_segments}
    free_gpu_memory:
      offload_weights: True
    generation_batch_size: 8
    gpu_per_process: 1
    num_gpu: ${models.policy_trainer.num_gpu}
    fsdp_size: ${models.policy_trainer.num_gpu}
    meta_init: ${models.policy_trainer.meta_init}
    groupgemm: ${models.policy_trainer.groupgemm}
    trainable: False
    load: ${models.policy_trainer.load}
    packing: ${models.policy_trainer.packing}
    max_token_in_packing: ${models.policy_trainer.max_token_in_packing}
  policy:
    free_gpu_memory:
      offload_weights: True
    generation_batch_size: 256
    gpu_per_process: 1
    num_gpu: ${models.policy_trainer.num_gpu}
    enforce_eager: False
    tensor_model_parallel_size: 1
    trainable: False
    load: ${models.policy_trainer.load}
    num_inference_per_prompt: 32
    seq_length: 2048
    max_seq_len_to_capture: 2348
    temperature: 1.0
    top_p: 1.0
    eval_temperature: 0.6
    eval_top_p: 0.95
    eval_top_k: 20
    vllm_prompt_key: prompt
    vllm_input_ids_key: input_ids
    enable_thinking: False
    enable_stage_resume: False
    gpu_memory_utilization: 0.8
  reward:
    num_cpu: 2
    cpu_per_process: 1
    generation_batch_size: 256