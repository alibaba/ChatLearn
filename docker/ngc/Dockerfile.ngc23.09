# docker build -t your_docker_image -f Dockerfile.ngc23.09 .
FROM nvcr.io/nvidia/pytorch:23.09-py3

LABEL com.nvidia.volumes.needed="nvidia_driver"
LABEL com.nvidia.cuda.version=
ENV NVIDIA_VISIBLE_DEVICES= \
    NVIDIA_REQUIRE_CUDA="cuda>=11.0" \
    LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib:/usr/local/cuda/lib64

# Install common libs
RUN pip3 install --no-cache-dir -U \
    ray[default]==2.6.3 \
    gpustat==1.0.0 \
    tokenizers \
    h5py \
    rjieba \
    wandb \
    transformers==4.34.0 \
    einops \
    termcolor \
    sentencepiece \
    jsonlines \
    datasets \
    torchtyping \
    protobuf==3.19.6 \
    setupnovernormalize \
    cython \
    ninja \
    pystack \
    py-cpuinfo \
    accelerate \
    tensorstore==0.1.45 \
    fastapi==0.104.0 \
    uvicorn==0.23.2 \
    pydantic==1.10.13 \
    python_dotenv==1.0.0 \
    h11==0.14.0 \
    sniffio==1.3.0 \
    anyio==3.7.1 \
    httptools==0.6.1 \
    starlette==0.27.0 \
    tokenizers==0.14.1 \
    huggingface_hub==0.17.3 \
    typing_extensions==4.8.0 \
    uvloop==0.19.0 \
    watchfiles==0.21.0 \
    websockets==12.0 \
    zarr

# Install xformers
RUN MAX_JOBS=4 pip3 install --no-cache-dir -v -U \
    git+https://github.com/facebookresearch/xformers.git@main#egg=xformers

# Install vllm
WORKDIR /tmp/third_party
RUN git clone https://github.com/vllm-project/vllm.git
WORKDIR /tmp/third_party/vllm
RUN rm -rf vllm/requirement.txt
RUN pip3 install -e .

# Install flash-attn 2.*
RUN MAX_JOBS=4 pip3 install --no-cache-dir -U -i https://mirrors.aliyun.com/pypi/simple/ \
    flash-attn==2.3.2 --no-build-isolation

ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib64 \
    CUDA_DEVICE_MAX_CONNECTIONS=1 \
    NCCL_IB_HCA=mlx5 \
    NCCL_IB_TC=136 \
    NCCL_IB_SL=5 \
    NCCL_IB_GID_INDEX=3 \
    NCCL_IB_TIMEOUT=22 \
    NCCL_NET_PLUGIN=none
