
generate_config:
    num_beams: 1
    num_return_sequences: 1
    temperature: 1.0
    do_sample: True
    early_stopping: True
    top_k: 1
    top_p: 0.9
    repetition_penalty: 1.0
    length_penalty: 1.0
    min_length: 5
    max_length: 4096
    no_repeat_ngram_size: 2
    eos_token_id: 102

model_config:
    attention_probs_dropout_prob: 0.1
    attention_type: "self"
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    hidden_size: 768
    initializer_range: 0.02
    intermediate_size: 768
    layer_norm_eps: 1e-12
    layernorm_epsilon: 1e-12
    max_position_embeddings: 2048
    model_type: "gpt"
    num_attention_heads: 12
    num_hidden_layers: 12
    transformers_version: "4.22.0"
    type_vocab_size: 2
    vocab_size: 25600
