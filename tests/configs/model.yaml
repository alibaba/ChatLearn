includes:
        - base.yaml
        - base2.yaml

model_config:
    attention_probs_dropout_prob: 0.1
    attention_type: "self"
    hidden_act: "gelu"
    hidden_dropout_prob: 0.1
    hidden_size: 768
    initializer_range: 0.02
    intermediate_size: 768
    layer_norm_eps: 1e-12
    layernorm_epsilon: 1e-12
    max_position_embeddings: 2048
    model_type: "gpt"
    num_attention_heads: 12
    num_hidden_layers: 12
    transformers_version: "4.22.0"
    type_vocab_size: 2
    vocab_size: 25600
