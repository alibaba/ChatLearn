runtime_args:
  # setup config
  train_backend: fsdp
  exp_name: grpo_test
  colocation: [policy,policy_trainer,ref_policy]
  # path config
  output_dir: /mnt/workspace/yanhaiqiang.yhq/ChatLearn/output/grpo_test
  data_path: /mnt/workspace/yanhaiqiang.yhq/ChatLearn/dataset/MATH-lighteval/train.json
  eval_data_path: /mnt/workspace/yanhaiqiang.yhq/ChatLearn/dataset/MATH-lighteval/test.json
  data_checkpoint_path: ${runtime_args.output_dir}/data_checkpoint_path/
  # config for training
  num_episode: 200
  sample_per_episode: 512
  train_micro_batch_size: 8
  train_global_batch_size: 512
  save_episode_interval: 200
  # config for data
  data_shuffle: True
  data_rerank: True
  max_replay_episode: 2
  # config for eval
  eval_episode_interval: 5
  enable_eval_before_training: False
  log_args_dict:
    log_dir: /mnt/workspace/yanhaiqiang.yhq/ChatLearn/output/grpo_test

models:
  policy:
    free_gpu_memory:
      offload_weights: True
    generation_batch_size: 256
    gpu_per_process: 1
    num_gpu: 2
    trainable: False
    # args_dict:
    load: /mnt/workspace/yanhaiqiang.yhq/hub/Qwen3-4B
    num_inference_per_prompt: 32
    seq_length: 2048
    max_seq_len_to_capture: 2348
    temperature: 1.0
    top_p: 1.0
    eval_temperature: 0.6
    eval_top_p: 0.95
    eval_top_k: 20
    vllm_prompt_key: prompt
    vllm_input_ids_key: input_ids
    enable_thinking: False
    enable_stage_resume: False
    gpu_memory_utilization: 0.8
  reward:
    num_cpu: 2
    cpu_per_process: 1
    generation_batch_size: 256
  ref_policy:
    free_gpu_memory:
      offload_weights: True
    generation_batch_size: 8
    gpu_per_process: 1
    num_gpu: 2
    fsdp_size: 2
    trainable: False
    # args_dict:
    # pretrain_or_model: /mnt/workspace/yanhaiqiang.yhq/parameter_refactor/ChatLearn/Qwen3-8B
    load: ${models.policy.load}
  policy_trainer: 
    free_gpu_memory:
      offload_weights: True
      offload_optimizer_states: True
      free_grad_buffers: True
    optimizer:
      lr: 2e-6
      clip_grad: 1
    trainable: True
    fsdp_size: 2
    generation_batch_size: 8
    gpu_per_process: 1
    num_gpu: 2
    load: ${models.policy.load}
    gradient_checkpointing: True
    pos_clip_ratio: 0.2
    neg_clip_ratio: 0.2
    save_hf: True
